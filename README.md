This project implements a Retrieval-Augmented Generation (RAG) system using LlamaIndex and LlamaCPP, enabling intelligent question-answering over your custom data sources like documents, emails, or databases.

üìö Overview
Retrieval-Augmented Generation (RAG) combines the strengths of:

Information Retrieval: Search for relevant information in a knowledge base.
Language Model (LLM): Generate context-aware, natural language responses.
In this project:

LlamaIndex is used to index and query documents.
LlamaCPP powers the language model for text generation.
üõ†Ô∏è Tech Stack
LlamaIndex: Indexing and querying framework.
LlamaCPP: Lightweight, efficient LLM runtime for local inference.
Python 3.10+: Programming language.
Hugging Face Models: Pre-trained models hosted on Hugging Face Hub.
